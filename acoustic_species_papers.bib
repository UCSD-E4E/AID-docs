 
@Misc{,
  note     = {ZSCC: NoCitationData[s0]},
  title    = {Methods for sound noise reduction},
  abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Cornell Birdcall Identification},
  language = {en},
  url      = {https://kaggle.com/code/mauriciofigueiredo/methods-for-sound-noise-reduction},
  urldate  = {2023-08-07},
}

@InProceedings{,
  author = {recognition already with small amount of local data},
  title  = {Domain-specific neural networks improve automated bird sound},
  file   = {:C\:/Users/seanh/Desktop/mee314003-sup-0001-supplementary.pdf:PDF},
}

 
@Article{Maclean2023,
  author   = {Maclean, Kyle and Triguero, Isaac},
  journal  = {Applied Intelligence},
  title    = {Identifying bird species by their calls in {Soundscapes}},
  year     = {2023},
  issn     = {1573-7497},
  month    = mar,
  abstract = {In many real data science problems, it is common to encounter a domain mismatch between the training and testing datasets, which means that solutions designed for one may not transfer well to the other due to their differences. An example of such was in the BirdCLEF2021 Kaggle competition, where participants had to identify all bird species that could be heard in audio recordings. Thus, multi-label classifiers, capable of coping with domain mismatch, were required. In addition, classifiers needed to be resilient to a long-tailed (imbalanced) class distribution and weak labels. Throughout the competition, a diverse range of solutions based on convolutional neural networks were proposed. However, it is unclear how different solution components contribute to overall performance. In this work, we contextualise the problem with respect to the previously existing literature, analysing and discussing the choices made by the different participants. We also propose a modular solution architecture to empirically quantify the effects of different architectures. The results of this study provide insights into which components worked well for this challenge.},
  doi      = {10.1007/s10489-023-04486-8},
  file     = {:Maclean2023 - Identifying Bird Species by Their Calls in Soundscapes.pdf:PDF},
  keywords = {Multi-label classification, Signal processing, Domain mismatch, Convolutional neural networks},
  language = {en},
  url      = {https://doi.org/10.1007/s10489-023-04486-8},
  urldate  = {2023-08-08},
}

 
@Article{Lee2023,
  author   = {Lee, JoonHo and Lee, Gyemin},
  journal  = {Neurocomputing},
  title    = {Unsupervised domain adaptation based on the predictive uncertainty of models},
  year     = {2023},
  issn     = {0925-2312},
  month    = feb,
  pages    = {183--193},
  volume   = {520},
  abstract = {Unsupervised domain adaptation (UDA) aims to improve the prediction performance in the target domain under distribution shifts from the source domain. The key principle of UDA is to minimize the divergence between the source and the target domains. To follow this principle, many methods employ a domain discriminator to match the feature distributions. Some recent methods evaluate the discrepancy between two predictions on target samples to detect those that deviate from the source distribution. However, their performance is limited because they either match the marginal distributions or measure the divergence conservatively. In this paper, we present a novel UDA method that learns domain-invariant features that minimize the domain divergence. We propose model uncertainty as a measure of the domain divergence. Our UDA method based on model uncertainty (MUDA) adopts a Bayesian framework and provides an efficient way to evaluate model uncertainty by means of Monte Carlo dropout sampling. Experiment results on image recognition tasks show that our method is superior to existing state-of-the-art methods. We also extend MUDA to multi-source domain adaptation problems.},
  doi      = {10.1016/j.neucom.2022.11.070},
  file     = {:Lee2023 - Unsupervised Domain Adaptation Based on the Predictive Uncertainty of Models.html:URL},
  keywords = {Unsupervised domain adaptation, Model uncertainty, Predictive variance, Monte Carlo dropout, Image classification},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231222014618},
  urldate  = {2023-08-07},
}

 
@TechReport{Fahrbach2023,
  author   = {Fahrbach, Matthew and Javanmard, Adel and Mirrokni, Vahab and Worah, Pratik},
  title    = {Learning {Rate} {Schedules} in the {Presence} of {Distribution} {Shift}},
  year     = {2023},
  month    = mar,
  note     = {arXiv:2303.15634 [cs, math, stat] type: article},
  abstract = {We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedules and their cumulative regret.},
  annote   = {Comment: 33 pages, 6 figures},
  doi      = {10.48550/arXiv.2303.15634},
  file     = {:Fahrbach2023 - Learning Rate Schedules in the Presence of Distribution Shift.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2303.15634},
  urldate  = {2023-08-07},
}

 
@TechReport{Boudiaf2023,
  author   = {Boudiaf, Malik and Denton, Tom and van Merriënboer, Bart and Dumoulin, Vincent and Triantafillou, Eleni},
  title    = {In {Search} for a {Generalizable} {Method} for {Source} {Free} {Domain} {Adaptation}},
  year     = {2023},
  month    = jun,
  note     = {arXiv:2302.06658 [cs] type: article},
  abstract = {Source-free domain adaptation (SFDA) is compelling because it allows adapting an off-the-shelf model to a new domain using only unlabelled data. In this work, we apply existing SFDA techniques to a challenging set of naturally-occurring distribution shifts in bioacoustics, which are very different from the ones commonly studied in computer vision. We find existing methods perform differently relative to each other than observed in vision benchmarks, and sometimes perform worse than no adaptation at all. We propose a new simple method which outperforms the existing methods on our new shifts while exhibiting strong performance on a range of vision datasets. Our findings suggest that existing SFDA methods are not as generalizable as previously thought and that considering diverse modalities can be a useful avenue for designing more robust models.},
  annote   = {Comment: ICML 2023},
  file     = {:Boudiaf2023 - In Search for a Generalizable Method for Source Free Domain Adaptation.pdf:PDF},
  keywords = {Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2302.06658},
  urldate  = {2023-08-07},
}

 
@Article{Bittner2023,
  author   = {Bittner, Franca and Abeßer, Jakob},
  title    = {An {Introduction} to {Unsupervised} {Domain} {Adaptation} in {Sound} and {Music} {Processing}},
  year     = {2023},
  month    = apr,
  note     = {ZSCC: 0000000},
  abstract = {Common machine learning models require large amounts of training data with samples representing the intended application scenario. However, these models often do not generalize well to novel data distributions caused by variations of the expected conditions. Such a lack of robustness can lead to a significant decrease in the model performance. This issue is known as domain shift and can be caused in the case of audio data by deviations of microphone characteristics or acoustic environments between data from the source domain (training data) and target domain (test data). Unsupervised domain adaptation (UDA) aims to restore the model performance by transferring knowledge from labeled samples of the source domain to unlabeled samples of a related target domain. We first provide an overview over basics and general approaches of UDA. Then, we study UDA for two audio analysis tasks: sound event detection (SED) and automatic music transcription (AMT) of piano music. Our results show that domain shift caused by microphone mismatch has a greater impact on the model performance for SED than AMT. As a possible cause we suspect that while SED analyzes the full spectral envelope, AMT examines only the harmonic peaks whose positions are less affected by domain shift.},
  language = {en},
  url      = {https://publica.fraunhofer.de/handle/publica/445495},
  urldate  = {2023-08-14},
}

 
@Misc{2022,
  month    = jan,
  note     = {ZSCC: NoCitationData[s0]},
  title    = {Separating {Birdsong} in the {Wild} for {Classification}},
  year     = {2022},
  language = {en},
  url      = {https://ai.googleblog.com/2022/01/separating-birdsong-in-wild-for.html},
  urldate  = {2023-08-08},
}

 
@TechReport{Yan2022,
  author   = {Yan, Jing-ke and Wang, Xin and Wang, Qin and Qin, Qin and Li, Huang-he and Ye, Peng-fei and He, Yue-ping and Zeng, Jing},
  title    = {Domain {Shift}-oriented {Machine} {Anomalous} {Sound} {Detection} {Model} {Based} on {Self}-{Supervised} {Learning}},
  year     = {2022},
  month    = sep,
  note     = {ZSCC: 0000000 arXiv:2208.14812 [cs, eess] type: article},
  abstract = {Thanks to the development of deep learning, research on machine anomalous sound detection based on self-supervised learning has made remarkable achievements. However, there are differences in the acoustic characteristics of the test set and the training set under different operating conditions of the same machine (domain shifts). It is challenging for the existing detection methods to learn the domain shifts features stably with low computation overhead. To address these problems, we propose a domain shift-oriented machine anomalous sound detection model based on self-supervised learning (TranSelf-DyGCN) in this paper. Firstly, we design a time-frequency domain feature modeling network to capture global and local spatial and time-domain features, thus improving the stability of machine anomalous sound detection stability under domain shifts. Then, we adopt a Dynamic Graph Convolutional Network (DyGCN) to model the inter-dependence relationship between domain shifts features, enabling the model to perceive domain shifts features efficiently. Finally, we use a Domain Adaptive Network (DAN) to compensate for the performance decrease caused by domain shifts, making the model adapt to anomalous sound better in the self-supervised environment. The performance of the suggested model is validated on DCASE 2020 task 2 and DCASE 2022 task 2.},
  doi      = {10.48550/arXiv.2208.14812},
  file     = {:Yan2022 - Domain Shift Oriented Machine Anomalous Sound Detection Model Based on Self Supervised Learning.pdf:PDF},
  keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2208.14812},
  urldate  = {2023-08-14},
}

@misc{wang2022debiased,
      title={Debiased Learning from Naturally Imbalanced Pseudo-Labels}, 
      author={Xudong Wang and Zhirong Wu and Long Lian and Stella X. Yu},
      year={2022},
      eprint={2201.01490},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Tang2022,
  author     = {Tang, Tiantian and Long, Yanhua and Li, Yijie and Liang, Jiaen},
  journal    = {Int. J. Speech Technol.},
  title      = {Acoustic Domain Mismatch Compensation in Bird Audio Detection},
  year       = {2022},
  issn       = {1381-2416},
  month      = mar,
  number     = {1},
  pages      = {251--260},
  volume     = {25},
  abstract   = {Detecting bird calls in audio is an important task for automatic wildlife monitoring, as well as in citizen science and audio library management. This paper presents front-end acoustic enhancement techniques to handle the acoustic domain mismatch problem in bird detection. A time-domain cross-condition data augmentation (TCDA) method is first proposed to enhance the domain coverage of a fixed training dataset. Then, to eliminate the distortion of stationary noise and enhance the transient events, we investigate a per-channel energy normalization (PCEN) to automatic control the gain of every subband in the mel-frequency spectrogram. Furthermore, a harmonic percussive source separation is investigated to extract robust percussive representations of bird call to alleviate the acoustic mismatch. Our experiments are performed on the Bird Audio Detection Task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events 2018. Extensive results show that the proposed TCDA leads to a relative 5.02% AUC improvements on mismatch conditions. And also on the cross-domain test set, the proposed percussive features (RPFs), and these RPFs with PCEN significantly improve the baseline with conventional log mel-spectrogram features from 81.79% AUC to 84.46% and 88.68%, respectively. Moreover, we find that combing different front-end features can further improve the system performances.},
  address    = {Berlin, Heidelberg},
  doi        = {10.1007/s10772-022-09957-w},
  issue_date = {Mar 2022},
  keywords   = {Bird audio detection, Domain mismatch, Harmonic percussive source separation, Data augmentation},
  numpages   = {10},
  publisher  = {Springer-Verlag},
  url        = {https://doi.org/10.1007/s10772-022-09957-w},
}

 
@TechReport{Nagesh2022,
  author     = {Nagesh, Chandra Kanth and Purushothama, Abhishek},
  title      = {The {Birds} {Need} {Attention} {Too}: {Analysing} usage of {Self} {Attention} in identifying bird calls in soundscapes},
  year       = {2022},
  month      = nov,
  note       = {arXiv:2211.07722 [cs, eess] type: article},
  abstract   = {Birds are vital parts of ecosystems across the world and are an excellent measure of the quality of life on earth. Many bird species are endangered while others are already extinct. Ecological efforts in understanding and monitoring bird populations are important to conserve their habitat and species, but this mostly relies on manual methods in rough terrains. Recent advances in Machine Learning and Deep Learning have made automatic bird recognition in diverse environments possible. Birdcall recognition till now has been performed using convolutional neural networks. In this work, we try and understand how self-attention can aid in this endeavor. With that we build an pre-trained Attention-based Spectrogram Transformer baseline for BirdCLEF 2022 and compare the results against the pre-trained Convolution-based baseline. Our results show that the transformer models outperformed the convolutional model and we further validate our results by building baselines and analyzing the results for the previous year BirdCLEF 2021 challenge. Source code available at https://github.com/ck090/BirdCLEF-22},
  annote     = {Comment: 12 pages, 9 tables and 7 figures},
  file       = {:Nagesh2022 - The Birds Need Attention Too_ Analysing Usage of Self Attention in Identifying Bird Calls in Soundscapes.pdf:PDF},
  keywords   = {Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  school     = {arXiv},
  shorttitle = {The {Birds} {Need} {Attention} {Too}},
  url        = {http://arxiv.org/abs/2211.07722},
  urldate    = {2023-08-08},
}

@Article{Lauha2022,
  author   = {Lauha, Patrik and Somervuo, Panu and Lehikoinen, Petteri and Geres, Lisa and Richter, Tobias and Seibold, Sebastian and Ovaskainen, Otso},
  journal  = {Methods in Ecology and Evolution},
  title    = {Domain-specific neural networks improve automated bird sound recognition already with small amount of local data},
  year     = {2022},
  number   = {12},
  pages    = {2799--2810},
  volume   = {13},
  abstract = {Abstract An automatic bird sound recognition system is a useful tool for collecting data of different bird species for ecological analysis. Together with autonomous recording units (ARUs), such a system provides a possibility to collect bird observations on a scale that no human observer could ever match. During the last decades, progress has been made in the field of automatic bird sound recognition, but recognizing bird species from untargeted soundscape recordings remains a challenge. In this article, we demonstrate the workflow for building a global identification model and adjusting it to perform well on the data of autonomous recorders from a specific region. We show how data augmentation and a combination of global and local data can be used to train a convolutional neural network to classify vocalizations of 101 bird species. We construct a model and train it with a global data set to obtain a base model. The base model is then fine-tuned with local data from Southern Finland in order to adapt it to the sound environment of a specific location and tested with two data sets: one originating from the same Southern Finnish region and another originating from a different region in German Alps. Our results suggest that fine-tuning with local data significantly improves the network performance. Classification accuracy was improved for test recordings from the same area as the local training data (Southern Finland) but not for recordings from a different region (German Alps). Data augmentation enables training with a limited number of training data and even with few local data samples significant improvement over the base model can be achieved. Our model outperforms the current state-of-the-art tool for automatic bird sound classification. Using local data to adjust the recognition model for the target domain leads to improvement over general non-tailored solutions. The process introduced in this article can be applied to build a fine-tuned bird sound classification model for a specific environment.},
  doi      = {https://doi.org/10.1111/2041-210X.14003},
  eprint   = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.14003},
  keywords = {autonomous recording units, bioacoustics, bio-monitoring, bird sound recognition, convolutional neural networks, deep learning, model fine-tuning},
  url      = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.14003},
}

 
@TechReport{Conde2022,
  author   = {Conde, Marcos V. and Choi, Ui-Jin},
  title    = {Few-shot {Long}-{Tailed} {Bird} {Audio} {Recognition}},
  year     = {2022},
  month    = jul,
  note     = {ZSCC: 0000003 arXiv:2206.11260 [cs, eess] type: article},
  abstract = {It is easier to hear birds than see them. However, they still play an essential role in nature and are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Deep Neural Networks allow us to process audio data to detect and classify birds. This technology can assist researchers in monitoring bird populations and biodiversity. We propose a sound detection and classification pipeline to analyze complex soundscape recordings and identify birdcalls in the background. Our method learns from weak labels and few data and acoustically recognizes the bird species. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022 Challenge hosted on Kaggle.},
  annote   = {Comment: LifeCLEF2022 (best paper award)},
  file     = {:Conde2022 - Few Shot Long Tailed Bird Audio Recognition.pdf:PDF},
  keywords = {Computer Science - Sound, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2206.11260},
  urldate  = {2023-08-07},
}

 
@TechReport{Liu2021,
  author     = {Liu, Xiaofeng and Guo, Zhenhua and Li, Site and Xing, Fangxu and You, Jane and Kuo, C.-C. Jay and Fakhri, Georges El and Woo, Jonghye},
  title      = {Adversarial {Unsupervised} {Domain} {Adaptation} with {Conditional} and {Label} {Shift}: {Infer}, {Align} and {Iterate}},
  year       = {2021},
  month      = aug,
  note       = {ZSCC: 0000048 arXiv:2107.13469 [cs] type: article},
  abstract   = {In this work, we propose an adversarial unsupervised domain adaptation (UDA) approach with the inherent conditional and label shifts, in which we aim to align the distributions w.r.t. both \$p(x{\textbar}y)\$ and \$p(y)\$. Since the label is inaccessible in the target domain, the conventional adversarial UDA assumes \$p(y)\$ is invariant across domains, and relies on aligning \$p(x)\$ as an alternative to the \$p(x{\textbar}y)\$ alignment. To address this, we provide a thorough theoretical and empirical analysis of the conventional adversarial UDA methods under both conditional and label shifts, and propose a novel and practical alternative optimization scheme for adversarial UDA. Specifically, we infer the marginal \$p(y)\$ and align \$p(x{\textbar}y)\$ iteratively in the training, and precisely align the posterior \$p(y{\textbar}x)\$ in testing. Our experimental results demonstrate its effectiveness on both classification and segmentation UDA, and partial UDA.},
  annote     = {Comment: Accepted to ICCV 2021},
  file       = {:Liu2021 - Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift_ Infer, Align and Iterate.pdf:PDF},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multimedia},
  school     = {arXiv},
  shorttitle = {Adversarial {Unsupervised} {Domain} {Adaptation} with {Conditional} and {Label} {Shift}},
  url        = {http://arxiv.org/abs/2107.13469},
  urldate    = {2023-08-15},
}

 
@Article{Jahani2021,
  author   = {Jahani, Ali and Kalantary, Saba and Alitavoli, Asal},
  journal  = {Urban Forestry \& Urban Greening},
  title    = {An application of artificial intelligence techniques in prediction of birds soundscape impact on tourists’ mental restoration in natural urban areas},
  year     = {2021},
  issn     = {1618-8667},
  month    = jun,
  pages    = {127088},
  volume   = {61},
  abstract = {The characteristics of birds' sounds assume a primary role in tourists' mental restoration and stress recovery. The aim of this research is the evaluation of birds' sound composition in mental restoration of urban tourists to develop a decision support system as a practical tool. In this order, the recorded sounds of six birds were composed (57 composed sounds) and the human perception approach was used to assess the impact of sounds on the urban tourist's mental restoration. The MLP (Multi-Layer Perceptron), RBFNN (Radial Basis Function Neural Network) and SVM (Support Vector Machine) models were developed for mental restoration prediction in different birds' sound compositions. The results indicated that RBFNN model output (R2 training = 0.89, and R2 test = 0.85) has the best accuracy compared to the MLP and SVM models in prediction of birds' soundscape score in natural urban areas. According to the sensitivity analysis, the values of White eared Bulbul (Pycononotus leucotis), Great Tit (Parus major), House Sparrow (Passer domesticus), Laughing Dove (Spilopelia senegalensis), White Wagtail (Motacilla alba), and Eurasian Magpie (Pica pica) are prioritized respectively that influence the RBFNN model outputs. In practice, the designed environmental decision support system tool is applied by urban planners, managers, psychoacoustic researchers, and landscape architects to predict the landscape score in different birds' habitats.},
  doi      = {10.1016/j.ufug.2021.127088},
  file     = {:Jahani2021 - An Application of Artificial Intelligence Techniques in Prediction of Birds Soundscape Impact on Tourists’ Mental Restoration in Natural Urban Areas.html:URL},
  keywords = {Mental restoration, Multi-Layer Perceptron, Radial Basis Function Neural Network, Soundscape, Stress recovery, Support Vector Machine},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S1618866721001138},
  urldate  = {2023-08-08},
}

 
@TechReport{Denton2021,
  author   = {Denton, Tom and Wisdom, Scott and Hershey, John R.},
  title    = {Improving {Bird} {Classification} with {Unsupervised} {Sound} {Separation}},
  year     = {2021},
  month    = oct,
  note     = {arXiv:2110.03209 [eess] type: article},
  abstract = {This paper addresses the problem of species classification in bird song recordings. The massive amount of available field recordings of birds presents an opportunity to use machine learning to automatically track bird populations. However, it also poses a problem: such field recordings typically contain significant environmental noise and overlapping vocalizations that interfere with classification. The widely available training datasets for species identification also typically leave background species unlabeled. This leads classifiers to ignore vocalizations with a low signal-to-noise ratio. However, recent advances in unsupervised sound separation, such as {\textbackslash}emph\{mixture invariant training\} (MixIT), enable high quality separation of bird songs to be learned from such noisy recordings. In this paper, we demonstrate improved separation quality when training a MixIT model specifically for birdsong data, outperforming a general audio separation model by over 5 dB in SI-SNR improvement of reconstructed mixtures. We also demonstrate precision improvements with a downstream multi-species bird classifier across three independent datasets. The best classifier performance is achieved by taking the maximum model activations over the separated channels and original audio. Finally, we document additional classifier improvements, including taxonomic classification, augmentation by random low-pass filters, and additional channel normalization.},
  annote   = {Comment: 5 pages, 3 figures. Examples available at https://bird-mixit.github.io},
  file     = {:Denton2021 - Improving Bird Classification with Unsupervised Sound Separation.pdf:PDF},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2110.03209},
  urldate  = {2023-08-07},
}

 
@TechReport{Conde2021,
  author   = {Conde, Marcos V. and Shubham, Kumar and Agnihotri, Prateek and Movva, Nitin D. and Bessenyei, Szilard},
  title    = {Weakly-{Supervised} {Classification} and {Detection} of {Bird} {Sounds} in the {Wild}. {A} {BirdCLEF} 2021 {Solution}},
  year     = {2021},
  month    = jul,
  note     = {arXiv:2107.04878 [cs, eess] type: article},
  abstract = {It is easier to hear birds than see them, however, they still play an essential role in nature and they are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Machine Learning and Convolutional Neural Networks allow us to detect and classify bird sounds, by doing this, we can assist researchers in monitoring the status and trends of bird populations and biodiversity in ecosystems. We propose a sound detection and classification pipeline for analyzing complex soundscape recordings and identify birdcalls in the background. Our pipeline learns from weak labels, classifies fine-grained bird vocalizations in the wild, and is robust against background sounds (e.g., airplanes, rain, etc). Our solution achieved 10th place of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.},
  annote   = {Comment: Proceedings Working Notes CEURWS @ CLEF 2021 - BirdCLEF 2021},
  file     = {:Conde2021 - Weakly Supervised Classification and Detection of Bird Sounds in the Wild. a BirdCLEF 2021 Solution.pdf:PDF},
  keywords = {Computer Science - Sound, Computer Science - Multimedia, Electrical Engineering and Systems Science - Audio and Speech Processing},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2107.04878},
  urldate  = {2023-08-07},
}

 
@TechReport{Abesser2021,
  author   = {Abeßer, Jakob and Müller, Meinard},
  title    = {Towards {Audio} {Domain} {Adaptation} for {Acoustic} {Scene} {Classification} using {Disentanglement} {Learning}},
  year     = {2021},
  month    = oct,
  note     = {ZSCC: 0000004 arXiv:2110.13586 [cs, eess] type: article},
  abstract = {The deployment of machine listening algorithms in real-life applications is often impeded by a domain shift caused for instance by different microphone characteristics. In this paper, we propose a novel domain adaptation strategy based on disentanglement learning. The goal is to disentangle task-specific and domain-specific characteristics in the analyzed audio recordings. In particular, we combine two strategies: First, we apply different binary masks to internal embedding representations and, second, we suggest a novel combination of categorical cross-entropy and variance-based losses. Our results confirm the disentanglement of both tasks on an embedding level but show only minor improvement in the acoustic scene classification performance, when training data from both domains can be used. As a second finding, we can confirm the effectiveness of a state-of-the-art unsupervised domain adaptation strategy, which performs across-domain adaptation on a feature-level instead.},
  annote   = {Comment: submitted to ICASSP 2022},
  doi      = {10.48550/arXiv.2110.13586},
  file     = {:Abesser2021 - Towards Audio Domain Adaptation for Acoustic Scene Classification Using Disentanglement Learning.pdf:PDF},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2110.13586},
  urldate  = {2023-08-14},
}

 
@TechReport{Wisdom2020,
  author   = {Wisdom, Scott and Tzinis, Efthymios and Erdogan, Hakan and Weiss, Ron J. and Wilson, Kevin and Hershey, John R.},
  title    = {Unsupervised {Sound} {Separation} {Using} {Mixture} {Invariant} {Training}},
  year     = {2020},
  month    = oct,
  note     = {arXiv:2006.12701 [cs, eess] type: article},
  abstract = {In recent years, rapid progress has been made on the problem of single-channel sound separation using supervised training of deep neural networks. In such supervised approaches, a model is trained to predict the component sources from synthetic mixtures created by adding up isolated ground-truth sources. Reliance on this synthetic training data is problematic because good performance depends upon the degree of match between the training data and real-world audio, especially in terms of the acoustic conditions and distribution of sources. The acoustic properties can be challenging to accurately simulate, and the distribution of sound types may be hard to replicate. In this paper, we propose a completely unsupervised method, mixture invariant training (MixIT), that requires only single-channel acoustic mixtures. In MixIT, training examples are constructed by mixing together existing mixtures, and the model separates them into a variable number of latent sources, such that the separated sources can be remixed to approximate the original mixtures. We show that MixIT can achieve competitive performance compared to supervised methods on speech separation. Using MixIT in a semi-supervised learning setting enables unsupervised domain adaptation and learning from large amounts of real world data without ground-truth source waveforms. In particular, we significantly improve reverberant speech separation performance by incorporating reverberant mixtures, train a speech enhancement system from noisy mixtures, and improve universal sound separation by incorporating a large amount of in-the-wild data.},
  annote   = {Comment: Accepted for spotlight presentation at NeurIPS 2020},
  file     = {:Wisdom2020 - Unsupervised Sound Separation Using Mixture Invariant Training.pdf:PDF},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Sound},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2006.12701},
  urldate  = {2023-08-07},
}

 
@InProceedings{Kahl2020,
  author     = {Kahl, Stefan and Clapp, Mary and Hopping, W. and Goëau, Hervé and Glotin, Hervé and Planqué, Robert and Vellinga, Willem-Pier and Joly, Alexis},
  title      = {Overview of {BirdCLEF} 2020: {Bird} {Sound} {Recognition} in {Complex} {Acoustic} {Environments}},
  year       = {2020},
  month      = sep,
  abstract   = {Passive acoustic monitoring is a cornerstone of the assessment of ecosystem health and the improvement of automated assessment systems has the potential to have a transformative impact on global biodiversity monitoring, at a scale and level of detail that is impossible with manual annotation or other more traditional methods. The BirdCLEF challenge—as part of the 2020 LifeCLEF Lab [12]—focuses on the development of reliable detection systems for avian vocalizations in continuous soundscape data. The goal of the task is to localize and identify all audible birds within the provided soundscape test set. This paper describes the methodology of the conducted evaluation as well as the synthesis of the main results and lessons learned.},
  file       = {Full Text PDF:https\://www.researchgate.net/profile/W-Hopping/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments/links/5f978ce4a6fdccfd7b825b36/Overview-of-BirdCLEF-2020-Bird-Sound-Recognition-in-Complex-Acoustic-Environments.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments:},
  shorttitle = {Overview of {BirdCLEF} 2020},
  url        = {https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments},
}

 
@TechReport{Stacke2019,
  author   = {Stacke, Karin and Eilertsen, Gabriel and Unger, Jonas and Lundström, Claes},
  title    = {A {Closer} {Look} at {Domain} {Shift} for {Deep} {Learning} in {Histopathology}},
  year     = {2019},
  month    = sep,
  note     = {arXiv:1909.11575 [cs] type: article},
  abstract = {Domain shift is a significant problem in histopathology. There can be large differences in data characteristics of whole-slide images between medical centers and scanners, making generalization of deep learning to unseen data difficult. To gain a better understanding of the problem, we present a study on convolutional neural networks trained for tumor classification of H\&E stained whole-slide images. We analyze how augmentation and normalization strategies affect performance and learned representations, and what features a trained model respond to. Most centrally, we present a novel measure for evaluating the distance between domains in the context of the learned representation of a particular model. This measure can reveal how sensitive a model is to domain variations, and can be used to detect new data that a model will have problems generalizing to. The results show how learning is heavily influenced by the preparation of training data, and that the latent representation used to do classification is sensitive to changes in data distribution, especially when training without augmentation or normalization.},
  annote   = {Comment: 8 pages, 4 figures. Accepted to COMPAY2019: Second MICCAI Workshop on Computational Pathology},
  file     = {:Stacke2019 - A Closer Look at Domain Shift for Deep Learning in Histopathology.pdf:PDF},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1909.11575},
  urldate  = {2023-08-07},
}

@InProceedings{Lostanlen2018,
  author    = {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and Mcfee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan},
  title     = {Per-Channel Energy Normalization: Why and How},
  year      = {2018},
  publisher = {IEEE},
  abstract  = {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named perchannel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This article investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-field noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and find empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Secondly, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Thirdly, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise (AWGN), PCEN is a computationally efficient frontend for robust detection and classification of acoustic events in heterogeneous environments.},
  file      = {:C\:/Users/seanh/Desktop/lostanlen_pcen_spl2018.pdf:PDF},
  keywords  = {Acoustic noise, acoustic sensors, acoustic signal detection, signal classification, spectrogram},
}

 
@InProceedings{Adavanne2017,
  author    = {Adavanne, Sharath and Drossos, Konstantinos and Çakir, Emre and Virtanen, Tuomas},
  booktitle = {2017 25th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  title     = {Stacked convolutional and recurrent neural networks for bird audio detection},
  year      = {2017},
  month     = aug,
  note      = {ISSN: 2076-1465},
  pages     = {1729--1733},
  abstract  = {This paper studies the detection of bird calls in audio segments using stacked convolutional and recurrent neural networks. Data augmentation by blocks mixing and domain adaptation using a novel method of test mixing are proposed and evaluated in regard to making the method robust to unseen data. The contributions of two kinds of acoustic features (dominant frequency and log mel-band energy) and their combinations are studied in the context of bird audio detection. Our best achieved AUC measure on five cross-validations of the development data is 95.5\% and 88.1\% on the unseen evaluation data.},
  doi       = {10.23919/EUSIPCO.2017.8081505},
  file      = {:Adavanne2017 - Stacked Convolutional and Recurrent Neural Networks for Bird Audio Detection.pdf:PDF},
  issn      = {2076-1465},
  keywords  = {Birds, Feature extraction, Training, Recurrent neural networks, Harmonic analysis},
}

@InProceedings{Baktashmotlagh2016,
  author   = {Baktashmotlagh, Mahsa and Salzmann, Mathieu and Cvlab and Dogan, Urun and Kloft, Marius and Orabona, Francesco and Tommasi, Tatiana},
  title    = {Distribution-Matching Embedding for Visual Domain Adaptation},
  year     = {2016},
  abstract = {Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Distribution-Matching Embedding approach: An unsupervised domain adaptation method that overcomes this issue by mapping the data to a latent space where the distance between the empirical distributions of the source and target examples is minimized. In other words, we seek to extract the information that is invariant across the source and target data. In particular, we study two different distances to compare the source and target distributions: the Maximum Mean Discrepancy and the Hellinger distance. Furthermore, we show that our approach allows us to learn either a linear embedding, or a nonlinear one. We demonstrate the benefits of our approach on the tasks of visual object recognition, text categorization, and WiFi localization.},
  file     = {:C\:/Users/seanh/Desktop/15-207.pdf:PDF},
  keywords = {Domain Adaptation, Maximum Mean Discrepancy, Hellinger Distance, Distribution Matching, Domain Invariant Representations},
  url      = {http://jmlr.org/papers/v17/15-207.html},
}

 
@Article{Singer2015,
  author   = {Singer, Elliot and Reynolds, Douglas A.},
  journal  = {IEEE Signal Processing Letters},
  title    = {Domain {Mismatch} {Compensation} for {Speaker} {Recognition} {Using} a {Library} of {Whiteners}},
  year     = {2015},
  issn     = {1558-2361},
  month    = nov,
  number   = {11},
  pages    = {2000--2003},
  volume   = {22},
  abstract = {The development of the i-vector framework for generating low dimensional representations of speech utterances has led to considerable improvements in speaker recognition performance. Although these gains have been achieved in periodic National Institute of Standards and Technology (NIST) evaluations, the problem of domain mismatch, where the system development data and the application data are collected from different sources, remains a challenging one. The impact of domain mismatch was a focus of the Johns Hopkins University (JHU) 2013 speaker recognition workshop, where a domain adaptation challenge (DAC13) corpus was created to address this problem. This paper proposes an approach to domain mismatch compensation for applications where in-domain development data is assumed to be unavailable. The method is based on a generalization of data whitening used in association with i-vector length normalization and utilizes a library of whitening transforms trained at system development time using strictly out-of-domain data. The approach is evaluated on the 2013 domain adaptation challenge task and is shown to compare favorably to in-domain conventional whitening and to nuisance attribute projection (NAP) inter-dataset variability compensation.},
  doi      = {10.1109/LSP.2015.2451591},
  file     = {:Singer2015 - Domain Mismatch Compensation for Speaker Recognition Using a Library of Whiteners.pdf:PDF},
  keywords = {Speaker recognition, Covariance matrices, Libraries, Speech, NIST, Conferences, Computational modeling, Channel compensation, domain mismatch, i-vectors, whitening},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: protectedFlag:true;}

@Comment{jabref-meta: saveActions:enabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}

@Comment{jabref-meta: saveOrderConfig:specified;year;true;author;true;title;true;}
